{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Name : Zaraar Malik**\n",
    "# **Roll No # 21i-2705**\n",
    "# **Section : BS-AI-A**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Exploration Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:25.901771Z",
     "iopub.status.busy": "2024-11-15T11:06:25.901066Z",
     "iopub.status.idle": "2024-11-15T11:06:30.536745Z",
     "shell.execute_reply": "2024-11-15T11:06:30.535960Z",
     "shell.execute_reply.started": "2024-11-15T11:06:25.901720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "os.mkdir('trained_Vision_Transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Finding Total Number of the Classes**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:30.538577Z",
     "iopub.status.busy": "2024-11-15T11:06:30.538159Z",
     "iopub.status.idle": "2024-11-15T11:06:30.559204Z",
     "shell.execute_reply": "2024-11-15T11:06:30.558335Z",
     "shell.execute_reply.started": "2024-11-15T11:06:30.538543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Classes :  51\n"
     ]
    }
   ],
   "source": [
    "path='/kaggle/input/hmdb-human-activity-recognition/HMDB_dataset'\n",
    "classes=os.listdir(path)\n",
    "print('Total Number of Classes : ',len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Finding Total Number of :**\n",
    "\n",
    "* Video Samples\n",
    "* Training Samples\n",
    "* Testing Samples\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:32.033544Z",
     "iopub.status.busy": "2024-11-15T11:06:32.032656Z",
     "iopub.status.idle": "2024-11-15T11:06:33.415372Z",
     "shell.execute_reply": "2024-11-15T11:06:33.414367Z",
     "shell.execute_reply.started": "2024-11-15T11:06:32.033481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Names</th>\n",
       "      <th>Total Video Samples</th>\n",
       "      <th>Total Training Samples</th>\n",
       "      <th>Total Testing Samples</th>\n",
       "      <th>Class Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kick_ball</td>\n",
       "      <td>128</td>\n",
       "      <td>103</td>\n",
       "      <td>25</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catch</td>\n",
       "      <td>102</td>\n",
       "      <td>72</td>\n",
       "      <td>30</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shoot_ball</td>\n",
       "      <td>131</td>\n",
       "      <td>91</td>\n",
       "      <td>40</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climb_stairs</td>\n",
       "      <td>112</td>\n",
       "      <td>82</td>\n",
       "      <td>30</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>punch</td>\n",
       "      <td>126</td>\n",
       "      <td>98</td>\n",
       "      <td>28</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class Names  Total Video Samples  Total Training Samples  \\\n",
       "0     kick_ball                  128                     103   \n",
       "1         catch                  102                      72   \n",
       "2    shoot_ball                  131                      91   \n",
       "3  climb_stairs                  112                      82   \n",
       "4         punch                  126                      98   \n",
       "\n",
       "   Total Testing Samples                                         Class Path  \n",
       "0                     25  /kaggle/input/hmdb-human-activity-recognition/...  \n",
       "1                     30  /kaggle/input/hmdb-human-activity-recognition/...  \n",
       "2                     40  /kaggle/input/hmdb-human-activity-recognition/...  \n",
       "3                     30  /kaggle/input/hmdb-human-activity-recognition/...  \n",
       "4                     28  /kaggle/input/hmdb-human-activity-recognition/...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/kaggle/input/hmdb-human-activity-recognition/HMDB_dataset'\n",
    "dic={}\n",
    "training=[]\n",
    "testing=[]\n",
    "dic['Class Names']=os.listdir(path)\n",
    "dic['Total Video Samples']=[len(os.listdir(path+'/'+i)) for i in classes]\n",
    "class_path=[path+'/'+i for i in classes]\n",
    "for i in classes:\n",
    "    train=0\n",
    "    test=0\n",
    "    for j in os.listdir(path+'/'+i):        \n",
    "        if 'training' in j:\n",
    "            train=train+1\n",
    "        else:\n",
    "            test=test+1\n",
    "    training.append(train)\n",
    "    testing.append(test)\n",
    "dic['Total Training Samples']=training\n",
    "dic['Total Testing Samples']=testing\n",
    "dic['Class Path']=class_path\n",
    "frame=pd.DataFrame(dic)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Preparing the Training, Validation and Testing Datasets**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:33.417069Z",
     "iopub.status.busy": "2024-11-15T11:06:33.416758Z",
     "iopub.status.idle": "2024-11-15T11:06:33.460992Z",
     "shell.execute_reply": "2024-11-15T11:06:33.460202Z",
     "shell.execute_reply.started": "2024-11-15T11:06:33.417037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path='/kaggle/input/hmdb-human-activity-recognition/HMDB_dataset'\n",
    "train_dict={}\n",
    "val_dict={}\n",
    "training=[]\n",
    "validation=[]\n",
    "train_label=[]\n",
    "val_label=[]\n",
    "class_path=[path+'/'+i for i in classes]\n",
    "for i in classes:\n",
    "    for j in os.listdir(path+'/'+i):        \n",
    "        if 'training' in j:\n",
    "            training.append(path+'/'+i+'/'+j)\n",
    "            train_label.append(i)\n",
    "        else:\n",
    "            validation.append(path+'/'+i+'/'+j)\n",
    "            val_label.append(i)            \n",
    "train_dict['Class Name']=train_label\n",
    "train_dict['File Path']=training\n",
    "train_dataset=pd.DataFrame(train_dict)\n",
    "train_dataset = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "val_dict['Class Name']=val_label\n",
    "val_dict['File Path']=validation\n",
    "val_dataset=pd.DataFrame(val_dict)\n",
    "\n",
    "val_data, test_data = train_test_split(val_dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "validation_dataset = pd.DataFrame(val_data)\n",
    "test_dataset = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:33.462565Z",
     "iopub.status.busy": "2024-11-15T11:06:33.462154Z",
     "iopub.status.idle": "2024-11-15T11:06:33.472206Z",
     "shell.execute_reply": "2024-11-15T11:06:33.471336Z",
     "shell.execute_reply.started": "2024-11-15T11:06:33.462513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Name</th>\n",
       "      <th>File Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walk</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walk</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eat</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stand</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class Name                                          File Path\n",
       "0       walk  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "1       walk  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "2       walk  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "3        eat  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "4      stand  /kaggle/input/hmdb-human-activity-recognition/..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:33.474944Z",
     "iopub.status.busy": "2024-11-15T11:06:33.474198Z",
     "iopub.status.idle": "2024-11-15T11:06:33.484530Z",
     "shell.execute_reply": "2024-11-15T11:06:33.483503Z",
     "shell.execute_reply.started": "2024-11-15T11:06:33.474877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Name</th>\n",
       "      <th>File Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>dive</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>draw_sword</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>sword</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>smoke</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>somersault</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class Name                                          File Path\n",
       "541         dive  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "1340  draw_sword  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "480        sword  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "165        smoke  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "248   somersault  /kaggle/input/hmdb-human-activity-recognition/..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:33.521822Z",
     "iopub.status.busy": "2024-11-15T11:06:33.521512Z",
     "iopub.status.idle": "2024-11-15T11:06:33.530684Z",
     "shell.execute_reply": "2024-11-15T11:06:33.529697Z",
     "shell.execute_reply.started": "2024-11-15T11:06:33.521767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Name</th>\n",
       "      <th>File Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>shoot_bow</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>climb</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>push</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>push</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>kick</td>\n",
       "      <td>/kaggle/input/hmdb-human-activity-recognition/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class Name                                          File Path\n",
       "1520  shoot_bow  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "621       climb  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "352        push  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "353        push  /kaggle/input/hmdb-human-activity-recognition/...\n",
       "513        kick  /kaggle/input/hmdb-human-activity-recognition/..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Encode Classes Using Label Encoder**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:34.170635Z",
     "iopub.status.busy": "2024-11-15T11:06:34.169814Z",
     "iopub.status.idle": "2024-11-15T11:06:34.175988Z",
     "shell.execute_reply": "2024-11-15T11:06:34.175065Z",
     "shell.execute_reply.started": "2024-11-15T11:06:34.170596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_classes=encoder.fit_transform(classes)\n",
    "vector_to_label={}\n",
    "label_to_vector={}\n",
    "for label,vector in zip(classes,encoded_classes):\n",
    "    vector_to_label[int(vector)]=label\n",
    "    label_to_vector[label]=int(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Sample Processing Code for Video**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:34.630758Z",
     "iopub.status.busy": "2024-11-15T11:06:34.630422Z",
     "iopub.status.idle": "2024-11-15T11:06:34.637253Z",
     "shell.execute_reply": "2024-11-15T11:06:34.636306Z",
     "shell.execute_reply.started": "2024-11-15T11:06:34.630724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def video_to_limited_frames(video_path, num_frames=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(total_frames // num_frames, 1)\n",
    "\n",
    "    frame_list=[]\n",
    "    frame_count=0\n",
    "    saved_frames=0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % interval == 0 and saved_frames < num_frames:\n",
    "            frame_list.append(frame)\n",
    "            saved_frames+=1\n",
    "        frame_count+=1\n",
    "    cap.release()\n",
    "    return frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:35.010976Z",
     "iopub.status.busy": "2024-11-15T11:06:35.010623Z",
     "iopub.status.idle": "2024-11-15T11:06:35.170250Z",
     "shell.execute_reply": "2024-11-15T11:06:35.169350Z",
     "shell.execute_reply.started": "2024-11-15T11:06:35.010942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Extracted Frames :  10\n"
     ]
    }
   ],
   "source": [
    "frame_list=video_to_limited_frames('/kaggle/input/hmdb-human-activity-recognition/HMDB_dataset/brush_hair/testing_10.avi',num_frames=10)\n",
    "print('Total Number of Extracted Frames : ',len(frame_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Custom Dataset Class**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:06:37.801620Z",
     "iopub.status.busy": "2024-11-15T11:06:37.801271Z",
     "iopub.status.idle": "2024-11-15T11:06:51.320483Z",
     "shell.execute_reply": "2024-11-15T11:06:51.319702Z",
     "shell.execute_reply.started": "2024-11-15T11:06:37.801586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, label_dict, feature_extractor, num_frames=40):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): A dataframe with paths to video files and corresponding labels.\n",
    "            label_dict (dict): A dictionary to map labels to vectors.\n",
    "            feature_extractor (ViTFeatureExtractor): A ViT feature extractor for preprocessing.\n",
    "            num_frames (int): Number of frames to extract per video.\n",
    "            transform (callable, optional): Optional transform to be applied on a frame.\n",
    "        \"\"\"\n",
    "        self.frame = dataframe\n",
    "        self.label_dict = label_dict\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "        \n",
    "    def _extract_frames(self, video_path):\n",
    "        \"\"\"\n",
    "        Extract a fixed number of frames from a video file.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        interval = max(total_frames // self.num_frames, 1)\n",
    "        \n",
    "        frame_list=[]\n",
    "        frame_count=0\n",
    "        saved_frames=0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % interval == 0 and saved_frames < self.num_frames:\n",
    "                frame_list.append(frame)\n",
    "                saved_frames+=1\n",
    "            frame_count+=1\n",
    "        cap.release()\n",
    "        return frame_list\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.frame.iloc[idx]['File Path']\n",
    "        label = self.frame.iloc[idx]['Class Name']\n",
    "        label = self.label_dict[label]\n",
    "\n",
    "        if label is None:\n",
    "            raise ValueError(f\"Label '{label}' not found in label_dict\")\n",
    "\n",
    "        frames = self._extract_frames(video_path)\n",
    "\n",
    "        # Apply feature extraction to frames, resizing as necessary for ViT\n",
    "        processed_frames = [ self.feature_extractor(images=frame, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)  for frame in frames ]\n",
    "\n",
    "        # Stack frames into a single tensor of shape (num_frames, C, H, W)\n",
    "        frames_tensor = torch.stack(processed_frames)\n",
    "        label_vector=[torch.tensor(label,dtype=torch.long) for i in range(len(frames_tensor))]\n",
    "        label_vector=torch.tensor(label_vector)\n",
    "        \n",
    "        one_hot_labels = F.one_hot(label_vector, num_classes=len(self.label_dict.keys()))\n",
    "        label_tensor=torch.tensor(one_hot_labels,dtype=torch.float32,requires_grad=True)\n",
    "        \n",
    "        return frames_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Importing the Vision Transformer**\n",
    "# **Importing the Feature Exractor for the Vision Transformer**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:37:26.551889Z",
     "iopub.status.busy": "2024-11-15T11:37:26.551191Z",
     "iopub.status.idle": "2024-11-15T11:37:26.942115Z",
     "shell.execute_reply": "2024-11-15T11:37:26.941330Z",
     "shell.execute_reply.started": "2024-11-15T11:37:26.551840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"  # or other ViT model\n",
    "\n",
    "# Load the feature extractor and model\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "num_classes = len(classes)  # Adjust based on your dataset\n",
    "model.classifier = nn.Sequential(\n",
    "\n",
    "    nn.Linear(model.classifier.in_features, 256),  # Second additional layer\n",
    "    nn.ReLU(),  # Activation function\n",
    "    \n",
    "    nn.Linear(256, num_classes),  # Final linear layer to output the cla1ss scores\n",
    ")\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith('classifier'):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True  # Only classifier layers should require gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Load Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:37:28.721652Z",
     "iopub.status.busy": "2024-11-15T11:37:28.720781Z",
     "iopub.status.idle": "2024-11-15T11:37:29.267834Z",
     "shell.execute_reply": "2024-11-15T11:37:29.266854Z",
     "shell.execute_reply.started": "2024-11-15T11:37:28.721610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH='/kaggle/input/latest_model_final/pytorch/default/1/ViT_Epoch_2.pth'\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Checking the Classifier Layers in the Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:37:29.463383Z",
     "iopub.status.busy": "2024-11-15T11:37:29.462833Z",
     "iopub.status.idle": "2024-11-15T11:37:29.469239Z",
     "shell.execute_reply": "2024-11-15T11:37:29.468329Z",
     "shell.execute_reply.started": "2024-11-15T11:37:29.463346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=51, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Checking Total Number of Trainable Parameters**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:37:30.241671Z",
     "iopub.status.busy": "2024-11-15T11:37:30.241307Z",
     "iopub.status.idle": "2024-11-15T11:37:30.250079Z",
     "shell.execute_reply": "2024-11-15T11:37:30.249117Z",
     "shell.execute_reply.started": "2024-11-15T11:37:30.241636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters :  86,008,627    |    Total Trainable Parameters : 209,971\n"
     ]
    }
   ],
   "source": [
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "print(\"Total Model Parameters :  \"+\"{:,}\".format(num_params),'   |   ',\"Total Trainable Parameters : \"+\"{:,}\".format(trainable_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Datasets and Dataloaders**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:37:31.102024Z",
     "iopub.status.busy": "2024-11-15T11:37:31.101540Z",
     "iopub.status.idle": "2024-11-15T11:37:31.108589Z",
     "shell.execute_reply": "2024-11-15T11:37:31.107645Z",
     "shell.execute_reply.started": "2024-11-15T11:37:31.101987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1 \n",
    "TRAIN_DATASET = VideoDataset(dataframe=train_dataset, label_dict=label_to_vector, feature_extractor=feature_extractor,num_frames=40)\n",
    "VAL_DATASET   = VideoDataset(dataframe=validation_dataset, label_dict=label_to_vector, feature_extractor=feature_extractor,num_frames=40)\n",
    "TEST_DATASET  = VideoDataset(dataframe=test_dataset, label_dict=label_to_vector, feature_extractor=feature_extractor,num_frames=40)\n",
    "\n",
    "TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=batch_size, shuffle=True)\n",
    "VAL_LOADER   = DataLoader(VAL_DATASET, batch_size=batch_size, shuffle=False)\n",
    "TEST_LOADER  = DataLoader(TEST_DATASET, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T11:37:31.450032Z",
     "iopub.status.busy": "2024-11-15T11:37:31.449217Z",
     "iopub.status.idle": "2024-11-15T11:37:31.564115Z",
     "shell.execute_reply": "2024-11-15T11:37:31.563354Z",
     "shell.execute_reply.started": "2024-11-15T11:37:31.449991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experimentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:35:33.693186Z",
     "iopub.status.busy": "2024-11-15T12:35:33.692344Z",
     "iopub.status.idle": "2024-11-15T17:04:10.501229Z",
     "shell.execute_reply": "2024-11-15T17:04:10.500140Z",
     "shell.execute_reply.started": "2024-11-15T12:35:33.693147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TRAINING-------------\n",
      "Epoch [1/5], Step [100/5215], Loss: 0.7832\n",
      "Epoch [1/5], Step [200/5215], Loss: 0.6472\n",
      "Epoch [1/5], Step [300/5215], Loss: 0.6902\n",
      "Epoch [1/5], Step [400/5215], Loss: 0.7593\n",
      "Epoch [1/5], Step [500/5215], Loss: 0.6833\n",
      "Epoch [1/5], Step [600/5215], Loss: 0.7291\n",
      "Epoch [1/5], Step [700/5215], Loss: 0.6583\n",
      "Epoch [1/5], Step [800/5215], Loss: 0.6489\n",
      "Epoch [1/5], Step [900/5215], Loss: 0.8157\n",
      "Epoch [1/5], Step [1000/5215], Loss: 0.7639\n",
      "Epoch [1/5], Step [1100/5215], Loss: 0.7728\n",
      "Epoch [1/5], Step [1200/5215], Loss: 0.6944\n",
      "Epoch [1/5], Step [1300/5215], Loss: 0.6959\n",
      "Epoch [1/5], Step [1400/5215], Loss: 0.8189\n",
      "Epoch [1/5], Step [1500/5215], Loss: 0.7746\n",
      "Epoch [1/5], Step [1600/5215], Loss: 0.6438\n",
      "Epoch [1/5], Step [1700/5215], Loss: 0.7088\n",
      "Epoch [1/5], Step [1800/5215], Loss: 0.6219\n",
      "Epoch [1/5], Step [1900/5215], Loss: 0.7965\n",
      "Epoch [1/5], Step [2000/5215], Loss: 0.7561\n",
      "Epoch [1/5], Step [2100/5215], Loss: 0.8366\n",
      "Epoch [1/5], Step [2200/5215], Loss: 0.8094\n",
      "Epoch [1/5], Step [2300/5215], Loss: 0.7279\n",
      "Epoch [1/5], Step [2400/5215], Loss: 0.7873\n",
      "Epoch [1/5], Step [2500/5215], Loss: 0.6666\n",
      "Epoch [1/5], Step [2600/5215], Loss: 0.6825\n",
      "Epoch [1/5], Step [2700/5215], Loss: 0.6940\n",
      "Epoch [1/5], Step [2800/5215], Loss: 0.8464\n",
      "Epoch [1/5], Step [2900/5215], Loss: 0.8302\n",
      "Epoch [1/5], Step [3000/5215], Loss: 0.6951\n",
      "Epoch [1/5], Step [3100/5215], Loss: 0.6989\n",
      "Epoch [1/5], Step [3200/5215], Loss: 0.7391\n",
      "Epoch [1/5], Step [3300/5215], Loss: 0.7017\n",
      "Epoch [1/5], Step [3400/5215], Loss: 0.8070\n",
      "Epoch [1/5], Step [3500/5215], Loss: 0.7893\n",
      "Epoch [1/5], Step [3600/5215], Loss: 0.8828\n",
      "Epoch [1/5], Step [3700/5215], Loss: 0.9145\n",
      "Epoch [1/5], Step [3800/5215], Loss: 0.7131\n",
      "Epoch [1/5], Step [3900/5215], Loss: 0.7632\n",
      "Epoch [1/5], Step [4000/5215], Loss: 0.8508\n",
      "Epoch [1/5], Step [4100/5215], Loss: 0.9580\n",
      "Epoch [1/5], Step [4200/5215], Loss: 0.9081\n",
      "Epoch [1/5], Step [4300/5215], Loss: 0.9023\n",
      "Epoch [1/5], Step [4400/5215], Loss: 0.8266\n",
      "Epoch [1/5], Step [4500/5215], Loss: 0.7416\n",
      "Epoch [1/5], Step [4600/5215], Loss: 0.9081\n",
      "Epoch [1/5], Step [4700/5215], Loss: 0.6284\n",
      "Epoch [1/5], Step [4800/5215], Loss: 0.7921\n",
      "Epoch [1/5], Step [4900/5215], Loss: 0.8448\n",
      "Epoch [1/5], Step [5000/5215], Loss: 0.8342\n",
      "Epoch [1/5], Step [5100/5215], Loss: 0.7122\n",
      "Epoch [1/5], Step [5200/5215], Loss: 0.6822\n",
      "-------------VAlIDAION-------------\n",
      "Epoch [1/5], Step [50/5215], Loss: 0.8534\n",
      "Epoch [1/5], Step [100/5215], Loss: 0.7152\n",
      "Epoch [1/5], Step [150/5215], Loss: 0.5600\n",
      "Epoch [1/5], Step [200/5215], Loss: 0.8770\n",
      "Epoch [1/5], Step [250/5215], Loss: 0.9748\n",
      "Epoch [1/5], Step [300/5215], Loss: 0.5525\n",
      "Epoch [1/5], Step [350/5215], Loss: 0.7251\n",
      "Epoch [1/5], Step [400/5215], Loss: 0.9919\n",
      "Epoch [1/5], Step [450/5215], Loss: 0.6854\n",
      "Epoch [1/5], Step [500/5215], Loss: 0.8625\n",
      "Epoch [1/5], Step [550/5215], Loss: 0.7797\n",
      "Epoch [1/5], Step [600/5215], Loss: 0.7408\n",
      "Epoch [1/5], Step [650/5215], Loss: 0.5945\n",
      "Epoch [1/5], Step [700/5215], Loss: 0.5655\n",
      "Epoch [1/5], Step [750/5215], Loss: 0.7791\n",
      "\n",
      "\n",
      "Epoch [1/5] , Training Loss: 0.7601,  Validation Loss:  , 0.7263\n",
      "Saving Model\n",
      "\n",
      "\n",
      "-------------TRAINING-------------\n",
      "Epoch [2/5], Step [100/5215], Loss: 0.6991\n",
      "Epoch [2/5], Step [200/5215], Loss: 0.6142\n",
      "Epoch [2/5], Step [300/5215], Loss: 0.4989\n",
      "Epoch [2/5], Step [400/5215], Loss: 0.5591\n",
      "Epoch [2/5], Step [500/5215], Loss: 0.6252\n",
      "Epoch [2/5], Step [600/5215], Loss: 0.5965\n",
      "Epoch [2/5], Step [700/5215], Loss: 0.7770\n",
      "Epoch [2/5], Step [800/5215], Loss: 0.7544\n",
      "Epoch [2/5], Step [900/5215], Loss: 0.7173\n",
      "Epoch [2/5], Step [1000/5215], Loss: 0.7027\n",
      "Epoch [2/5], Step [1100/5215], Loss: 0.5712\n",
      "Epoch [2/5], Step [1200/5215], Loss: 0.5337\n",
      "Epoch [2/5], Step [1300/5215], Loss: 0.6734\n",
      "Epoch [2/5], Step [1400/5215], Loss: 0.7131\n",
      "Epoch [2/5], Step [1500/5215], Loss: 0.6581\n",
      "Epoch [2/5], Step [1600/5215], Loss: 0.6779\n",
      "Epoch [2/5], Step [1700/5215], Loss: 0.6550\n",
      "Epoch [2/5], Step [1800/5215], Loss: 0.7474\n",
      "Epoch [2/5], Step [1900/5215], Loss: 0.6627\n",
      "Epoch [2/5], Step [2000/5215], Loss: 0.6506\n",
      "Epoch [2/5], Step [2100/5215], Loss: 0.6297\n",
      "Epoch [2/5], Step [2200/5215], Loss: 0.8019\n",
      "Epoch [2/5], Step [2300/5215], Loss: 0.6691\n",
      "Epoch [2/5], Step [2400/5215], Loss: 0.7214\n",
      "Epoch [2/5], Step [2500/5215], Loss: 0.7273\n",
      "Epoch [2/5], Step [2600/5215], Loss: 0.6679\n",
      "Epoch [2/5], Step [2700/5215], Loss: 0.6359\n",
      "Epoch [2/5], Step [2800/5215], Loss: 0.6304\n",
      "Epoch [2/5], Step [2900/5215], Loss: 0.7746\n",
      "Epoch [2/5], Step [3000/5215], Loss: 0.7878\n",
      "Epoch [2/5], Step [3100/5215], Loss: 0.7042\n",
      "Epoch [2/5], Step [3200/5215], Loss: 0.6891\n",
      "Epoch [2/5], Step [3300/5215], Loss: 0.8526\n",
      "Epoch [2/5], Step [3400/5215], Loss: 0.9271\n",
      "Epoch [2/5], Step [3500/5215], Loss: 0.7285\n",
      "Epoch [2/5], Step [3600/5215], Loss: 0.7707\n",
      "Epoch [2/5], Step [3700/5215], Loss: 0.7320\n",
      "Epoch [2/5], Step [3800/5215], Loss: 0.7685\n",
      "Epoch [2/5], Step [3900/5215], Loss: 0.7501\n",
      "Epoch [2/5], Step [4000/5215], Loss: 0.7630\n",
      "Epoch [2/5], Step [4100/5215], Loss: 0.6375\n",
      "Epoch [2/5], Step [4200/5215], Loss: 0.7291\n",
      "Epoch [2/5], Step [4300/5215], Loss: 0.9404\n",
      "Epoch [2/5], Step [4400/5215], Loss: 0.8141\n",
      "Epoch [2/5], Step [4500/5215], Loss: 0.7158\n",
      "Epoch [2/5], Step [4600/5215], Loss: 0.6523\n",
      "Epoch [2/5], Step [4700/5215], Loss: 0.6307\n",
      "Epoch [2/5], Step [4800/5215], Loss: 0.6383\n",
      "Epoch [2/5], Step [4900/5215], Loss: 0.6750\n",
      "Epoch [2/5], Step [5000/5215], Loss: 0.7752\n",
      "Epoch [2/5], Step [5100/5215], Loss: 0.7525\n",
      "Epoch [2/5], Step [5200/5215], Loss: 0.6465\n",
      "-------------VAlIDAION-------------\n",
      "Epoch [2/5], Step [50/5215], Loss: 0.9425\n",
      "Epoch [2/5], Step [100/5215], Loss: 0.7461\n",
      "Epoch [2/5], Step [150/5215], Loss: 0.6209\n",
      "Epoch [2/5], Step [200/5215], Loss: 0.9164\n",
      "Epoch [2/5], Step [250/5215], Loss: 0.8995\n",
      "Epoch [2/5], Step [300/5215], Loss: 0.6186\n",
      "Epoch [2/5], Step [350/5215], Loss: 0.7520\n",
      "Epoch [2/5], Step [400/5215], Loss: 0.9839\n",
      "Epoch [2/5], Step [450/5215], Loss: 0.7455\n",
      "Epoch [2/5], Step [500/5215], Loss: 0.8595\n",
      "Epoch [2/5], Step [550/5215], Loss: 0.7278\n",
      "Epoch [2/5], Step [600/5215], Loss: 0.8176\n",
      "Epoch [2/5], Step [650/5215], Loss: 0.5472\n",
      "Epoch [2/5], Step [700/5215], Loss: 0.5364\n",
      "Epoch [2/5], Step [750/5215], Loss: 0.7933\n",
      "\n",
      "\n",
      "Epoch [2/5] , Training Loss: 0.6985,  Validation Loss:  , 0.7424\n",
      "Saving Model\n",
      "\n",
      "\n",
      "-------------TRAINING-------------\n",
      "Epoch [3/5], Step [100/5215], Loss: 0.4519\n",
      "Epoch [3/5], Step [200/5215], Loss: 0.4476\n",
      "Epoch [3/5], Step [300/5215], Loss: 0.6412\n",
      "Epoch [3/5], Step [400/5215], Loss: 0.5971\n",
      "Epoch [3/5], Step [500/5215], Loss: 0.5228\n",
      "Epoch [3/5], Step [600/5215], Loss: 0.6567\n",
      "Epoch [3/5], Step [700/5215], Loss: 0.5508\n",
      "Epoch [3/5], Step [800/5215], Loss: 0.6074\n",
      "Epoch [3/5], Step [900/5215], Loss: 0.7005\n",
      "Epoch [3/5], Step [1000/5215], Loss: 0.6024\n",
      "Epoch [3/5], Step [1100/5215], Loss: 0.6997\n",
      "Epoch [3/5], Step [1200/5215], Loss: 0.5821\n",
      "Epoch [3/5], Step [1300/5215], Loss: 0.6205\n",
      "Epoch [3/5], Step [1400/5215], Loss: 0.5986\n",
      "Epoch [3/5], Step [1500/5215], Loss: 0.5707\n",
      "Epoch [3/5], Step [1600/5215], Loss: 0.5663\n",
      "Epoch [3/5], Step [1700/5215], Loss: 0.6474\n",
      "Epoch [3/5], Step [1800/5215], Loss: 0.6496\n",
      "Epoch [3/5], Step [1900/5215], Loss: 0.6899\n",
      "Epoch [3/5], Step [2000/5215], Loss: 0.7002\n",
      "Epoch [3/5], Step [2100/5215], Loss: 0.6942\n",
      "Epoch [3/5], Step [2200/5215], Loss: 0.7552\n",
      "Epoch [3/5], Step [2300/5215], Loss: 0.5611\n",
      "Epoch [3/5], Step [2400/5215], Loss: 0.5550\n",
      "Epoch [3/5], Step [2500/5215], Loss: 0.6880\n",
      "Epoch [3/5], Step [2600/5215], Loss: 0.4371\n",
      "Epoch [3/5], Step [2700/5215], Loss: 0.7268\n",
      "Epoch [3/5], Step [2800/5215], Loss: 0.6465\n",
      "Epoch [3/5], Step [2900/5215], Loss: 0.4703\n",
      "Epoch [3/5], Step [3000/5215], Loss: 0.5851\n",
      "Epoch [3/5], Step [3100/5215], Loss: 0.5551\n",
      "Epoch [3/5], Step [3200/5215], Loss: 0.6385\n",
      "Epoch [3/5], Step [3300/5215], Loss: 0.6388\n",
      "Epoch [3/5], Step [3400/5215], Loss: 0.8221\n",
      "Epoch [3/5], Step [3500/5215], Loss: 0.7685\n",
      "Epoch [3/5], Step [3600/5215], Loss: 0.8571\n",
      "Epoch [3/5], Step [3700/5215], Loss: 0.6408\n",
      "Epoch [3/5], Step [3800/5215], Loss: 0.6292\n",
      "Epoch [3/5], Step [3900/5215], Loss: 0.6701\n",
      "Epoch [3/5], Step [4000/5215], Loss: 0.6388\n",
      "Epoch [3/5], Step [4100/5215], Loss: 0.6300\n",
      "Epoch [3/5], Step [4200/5215], Loss: 0.7935\n",
      "Epoch [3/5], Step [4300/5215], Loss: 0.6642\n",
      "Epoch [3/5], Step [4400/5215], Loss: 0.6451\n",
      "Epoch [3/5], Step [4500/5215], Loss: 0.7564\n",
      "Epoch [3/5], Step [4600/5215], Loss: 0.6342\n",
      "Epoch [3/5], Step [4700/5215], Loss: 0.7032\n",
      "Epoch [3/5], Step [4800/5215], Loss: 0.7320\n",
      "Epoch [3/5], Step [4900/5215], Loss: 0.6286\n",
      "Epoch [3/5], Step [5000/5215], Loss: 0.7102\n",
      "Epoch [3/5], Step [5100/5215], Loss: 0.6453\n",
      "Epoch [3/5], Step [5200/5215], Loss: 0.7173\n",
      "-------------VAlIDAION-------------\n",
      "Epoch [3/5], Step [50/5215], Loss: 0.9183\n",
      "Epoch [3/5], Step [100/5215], Loss: 0.7503\n",
      "Epoch [3/5], Step [150/5215], Loss: 0.5993\n",
      "Epoch [3/5], Step [200/5215], Loss: 0.9250\n",
      "Epoch [3/5], Step [250/5215], Loss: 0.9930\n",
      "Epoch [3/5], Step [300/5215], Loss: 0.5178\n",
      "Epoch [3/5], Step [350/5215], Loss: 0.7282\n",
      "Epoch [3/5], Step [400/5215], Loss: 0.9727\n",
      "Epoch [3/5], Step [450/5215], Loss: 0.7262\n",
      "Epoch [3/5], Step [500/5215], Loss: 0.9034\n",
      "Epoch [3/5], Step [550/5215], Loss: 0.7558\n",
      "Epoch [3/5], Step [600/5215], Loss: 0.7737\n",
      "Epoch [3/5], Step [650/5215], Loss: 0.5810\n",
      "Epoch [3/5], Step [700/5215], Loss: 0.5037\n",
      "Epoch [3/5], Step [750/5215], Loss: 0.8130\n",
      "\n",
      "\n",
      "Epoch [3/5] , Training Loss: 0.6393,  Validation Loss:  , 0.7394\n",
      "Saving Model\n",
      "\n",
      "\n",
      "-------------TRAINING-------------\n",
      "Epoch [4/5], Step [100/5215], Loss: 0.5672\n",
      "Epoch [4/5], Step [200/5215], Loss: 0.3888\n",
      "Epoch [4/5], Step [300/5215], Loss: 0.6039\n",
      "Epoch [4/5], Step [400/5215], Loss: 0.5164\n",
      "Epoch [4/5], Step [500/5215], Loss: 0.5183\n",
      "Epoch [4/5], Step [600/5215], Loss: 0.3999\n",
      "Epoch [4/5], Step [700/5215], Loss: 0.4922\n",
      "Epoch [4/5], Step [800/5215], Loss: 0.4579\n",
      "Epoch [4/5], Step [900/5215], Loss: 0.5644\n",
      "Epoch [4/5], Step [1000/5215], Loss: 0.6451\n",
      "Epoch [4/5], Step [1100/5215], Loss: 0.4416\n",
      "Epoch [4/5], Step [1200/5215], Loss: 0.4173\n",
      "Epoch [4/5], Step [1300/5215], Loss: 0.5514\n",
      "Epoch [4/5], Step [1400/5215], Loss: 0.5993\n",
      "Epoch [4/5], Step [1500/5215], Loss: 0.5925\n",
      "Epoch [4/5], Step [1600/5215], Loss: 0.5814\n",
      "Epoch [4/5], Step [1700/5215], Loss: 0.5905\n",
      "Epoch [4/5], Step [1800/5215], Loss: 0.6900\n",
      "Epoch [4/5], Step [1900/5215], Loss: 0.6588\n",
      "Epoch [4/5], Step [2000/5215], Loss: 0.5003\n",
      "Epoch [4/5], Step [2100/5215], Loss: 0.5768\n",
      "Epoch [4/5], Step [2200/5215], Loss: 0.5947\n",
      "Epoch [4/5], Step [2300/5215], Loss: 0.5062\n",
      "Epoch [4/5], Step [2400/5215], Loss: 0.6130\n",
      "Epoch [4/5], Step [2500/5215], Loss: 0.4837\n",
      "Epoch [4/5], Step [2600/5215], Loss: 0.7989\n",
      "Epoch [4/5], Step [2700/5215], Loss: 0.5850\n",
      "Epoch [4/5], Step [2800/5215], Loss: 0.5655\n",
      "Epoch [4/5], Step [2900/5215], Loss: 0.5183\n",
      "Epoch [4/5], Step [3000/5215], Loss: 0.6384\n",
      "Epoch [4/5], Step [3100/5215], Loss: 0.6520\n",
      "Epoch [4/5], Step [3200/5215], Loss: 0.7061\n",
      "Epoch [4/5], Step [3300/5215], Loss: 0.5482\n",
      "Epoch [4/5], Step [3400/5215], Loss: 0.7062\n",
      "Epoch [4/5], Step [3500/5215], Loss: 0.6801\n",
      "Epoch [4/5], Step [3600/5215], Loss: 0.6718\n",
      "Epoch [4/5], Step [3700/5215], Loss: 0.6997\n",
      "Epoch [4/5], Step [3800/5215], Loss: 0.5886\n",
      "Epoch [4/5], Step [3900/5215], Loss: 0.6108\n",
      "Epoch [4/5], Step [4000/5215], Loss: 0.5318\n",
      "Epoch [4/5], Step [4100/5215], Loss: 0.7482\n",
      "Epoch [4/5], Step [4200/5215], Loss: 0.6931\n",
      "Epoch [4/5], Step [4300/5215], Loss: 0.5833\n",
      "Epoch [4/5], Step [4400/5215], Loss: 0.6924\n",
      "Epoch [4/5], Step [4500/5215], Loss: 0.6217\n",
      "Epoch [4/5], Step [4600/5215], Loss: 0.7254\n",
      "Epoch [4/5], Step [4700/5215], Loss: 0.6411\n",
      "Epoch [4/5], Step [4800/5215], Loss: 0.5759\n",
      "Epoch [4/5], Step [4900/5215], Loss: 0.6965\n",
      "Epoch [4/5], Step [5000/5215], Loss: 0.5685\n",
      "Epoch [4/5], Step [5100/5215], Loss: 0.6799\n",
      "Epoch [4/5], Step [5200/5215], Loss: 0.6949\n",
      "-------------VAlIDAION-------------\n",
      "Epoch [4/5], Step [50/5215], Loss: 0.8708\n",
      "Epoch [4/5], Step [100/5215], Loss: 0.6961\n",
      "Epoch [4/5], Step [150/5215], Loss: 0.5953\n",
      "Epoch [4/5], Step [200/5215], Loss: 0.9345\n",
      "Epoch [4/5], Step [250/5215], Loss: 1.0428\n",
      "Epoch [4/5], Step [300/5215], Loss: 0.5456\n",
      "Epoch [4/5], Step [350/5215], Loss: 0.7608\n",
      "Epoch [4/5], Step [400/5215], Loss: 1.0304\n",
      "Epoch [4/5], Step [450/5215], Loss: 0.6929\n",
      "Epoch [4/5], Step [500/5215], Loss: 0.8738\n",
      "Epoch [4/5], Step [550/5215], Loss: 0.6943\n",
      "Epoch [4/5], Step [600/5215], Loss: 0.7741\n",
      "Epoch [4/5], Step [650/5215], Loss: 0.5727\n",
      "Epoch [4/5], Step [700/5215], Loss: 0.5540\n",
      "Epoch [4/5], Step [750/5215], Loss: 0.8197\n",
      "\n",
      "\n",
      "Epoch [4/5] , Training Loss: 0.5939,  Validation Loss:  , 0.7392\n",
      "Saving Model\n",
      "\n",
      "\n",
      "-------------TRAINING-------------\n",
      "Epoch [5/5], Step [100/5215], Loss: 0.5410\n",
      "Epoch [5/5], Step [200/5215], Loss: 0.4386\n",
      "Epoch [5/5], Step [300/5215], Loss: 0.4983\n",
      "Epoch [5/5], Step [400/5215], Loss: 0.4739\n",
      "Epoch [5/5], Step [500/5215], Loss: 0.5780\n",
      "Epoch [5/5], Step [600/5215], Loss: 0.5284\n",
      "Epoch [5/5], Step [700/5215], Loss: 0.4429\n",
      "Epoch [5/5], Step [800/5215], Loss: 0.5454\n",
      "Epoch [5/5], Step [900/5215], Loss: 0.4766\n",
      "Epoch [5/5], Step [1000/5215], Loss: 0.5789\n",
      "Epoch [5/5], Step [1100/5215], Loss: 0.5261\n",
      "Epoch [5/5], Step [1200/5215], Loss: 0.5168\n",
      "Epoch [5/5], Step [1300/5215], Loss: 0.5620\n",
      "Epoch [5/5], Step [1400/5215], Loss: 0.5910\n",
      "Epoch [5/5], Step [1500/5215], Loss: 0.4784\n",
      "Epoch [5/5], Step [1600/5215], Loss: 0.6070\n",
      "Epoch [5/5], Step [1700/5215], Loss: 0.4490\n",
      "Epoch [5/5], Step [1800/5215], Loss: 0.6474\n",
      "Epoch [5/5], Step [1900/5215], Loss: 0.4081\n",
      "Epoch [5/5], Step [2000/5215], Loss: 0.6551\n",
      "Epoch [5/5], Step [2100/5215], Loss: 0.5350\n",
      "Epoch [5/5], Step [2200/5215], Loss: 0.5090\n",
      "Epoch [5/5], Step [2300/5215], Loss: 0.4751\n",
      "Epoch [5/5], Step [2400/5215], Loss: 0.5759\n",
      "Epoch [5/5], Step [2500/5215], Loss: 0.6262\n",
      "Epoch [5/5], Step [2600/5215], Loss: 0.5864\n",
      "Epoch [5/5], Step [2700/5215], Loss: 0.5971\n",
      "Epoch [5/5], Step [2800/5215], Loss: 0.5799\n",
      "Epoch [5/5], Step [2900/5215], Loss: 0.5447\n",
      "Epoch [5/5], Step [3000/5215], Loss: 0.5908\n",
      "Epoch [5/5], Step [3100/5215], Loss: 0.5112\n",
      "Epoch [5/5], Step [3200/5215], Loss: 0.6378\n",
      "Epoch [5/5], Step [3300/5215], Loss: 0.4923\n",
      "Epoch [5/5], Step [3400/5215], Loss: 0.5463\n",
      "Epoch [5/5], Step [3500/5215], Loss: 0.4951\n",
      "Epoch [5/5], Step [3600/5215], Loss: 0.5454\n",
      "Epoch [5/5], Step [3700/5215], Loss: 0.6450\n",
      "Epoch [5/5], Step [3800/5215], Loss: 0.5095\n",
      "Epoch [5/5], Step [3900/5215], Loss: 0.5381\n",
      "Epoch [5/5], Step [4000/5215], Loss: 0.5059\n",
      "Epoch [5/5], Step [4100/5215], Loss: 0.5940\n",
      "Epoch [5/5], Step [4200/5215], Loss: 0.5567\n",
      "Epoch [5/5], Step [4300/5215], Loss: 0.5584\n",
      "Epoch [5/5], Step [4400/5215], Loss: 0.4992\n",
      "Epoch [5/5], Step [4500/5215], Loss: 0.6680\n",
      "Epoch [5/5], Step [4600/5215], Loss: 0.5767\n",
      "Epoch [5/5], Step [4700/5215], Loss: 0.6631\n",
      "Epoch [5/5], Step [4800/5215], Loss: 0.4867\n",
      "Epoch [5/5], Step [4900/5215], Loss: 0.4285\n",
      "Epoch [5/5], Step [5000/5215], Loss: 0.6770\n",
      "Epoch [5/5], Step [5100/5215], Loss: 0.5119\n",
      "Epoch [5/5], Step [5200/5215], Loss: 0.5524\n",
      "-------------VAlIDAION-------------\n",
      "Epoch [5/5], Step [50/5215], Loss: 0.9374\n",
      "Epoch [5/5], Step [100/5215], Loss: 0.7230\n",
      "Epoch [5/5], Step [150/5215], Loss: 0.5455\n",
      "Epoch [5/5], Step [200/5215], Loss: 0.8810\n",
      "Epoch [5/5], Step [250/5215], Loss: 0.8664\n",
      "Epoch [5/5], Step [300/5215], Loss: 0.5561\n",
      "Epoch [5/5], Step [350/5215], Loss: 0.7161\n",
      "Epoch [5/5], Step [400/5215], Loss: 1.0069\n",
      "Epoch [5/5], Step [450/5215], Loss: 0.7319\n",
      "Epoch [5/5], Step [500/5215], Loss: 0.8712\n",
      "Epoch [5/5], Step [550/5215], Loss: 0.7584\n",
      "Epoch [5/5], Step [600/5215], Loss: 0.7764\n",
      "Epoch [5/5], Step [650/5215], Loss: 0.5340\n",
      "Epoch [5/5], Step [700/5215], Loss: 0.5393\n",
      "Epoch [5/5], Step [750/5215], Loss: 0.7834\n",
      "\n",
      "\n",
      "Epoch [5/5] , Training Loss: 0.5439,  Validation Loss:  , 0.7243\n",
      "Saving Model\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5\n",
    "SAVE_EPOCH_LOSS=[]\n",
    "SAVE_VAL_LOSS=[]\n",
    "\n",
    "model_name='ViT_Epoch'\n",
    "PATH='/kaggle/working/trained_Vision_Transformer/'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    epoch_loss=0.0\n",
    "    print('-------------TRAINING-------------')\n",
    "    for i, (frames, labels) in enumerate(TRAIN_LOADER):\n",
    "\n",
    "        frames = frames.to(device) \n",
    "        labels = labels.to(device)\n",
    "\n",
    " \n",
    "        batch_size, num_frames, C, H, W = frames.shape\n",
    "        frames = frames.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "   \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        logits = outputs.logits.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        logits=logits.squeeze(0)\n",
    "        labels=labels.squeeze(0)\n",
    "\n",
    "        loss = criterion(logits, labels)  \n",
    " \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(TRAIN_LOADER)}], Loss: {running_loss / 100:.4f}\")\n",
    "            epoch_loss=epoch_loss+running_loss\n",
    "            running_loss = 0.0\n",
    "    print('-------------VAlIDAION-------------')\n",
    "    model.eval()\n",
    "    val_loss=0.0\n",
    "    for i, (frames, labels) in enumerate (VAL_LOADER):\n",
    "        \n",
    "        frames = frames.to(device) \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        batch_size, num_frames, C, H, W = frames.shape\n",
    "        frames = frames.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "        outputs = model(frames)\n",
    "        logits = outputs.logits.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        logits=logits.squeeze(0)\n",
    "        labels=labels.squeeze(0)\n",
    "\n",
    "        loss = criterion(logits, labels)  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(TRAIN_LOADER)}], Loss: {running_loss / 50:.4f}\")\n",
    "            val_loss=val_loss+running_loss\n",
    "            running_loss = 0.0\n",
    "    print('\\n')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] , Training Loss: {epoch_loss / len(TRAIN_LOADER):.4f},  Validation Loss:  , {val_loss/len(VAL_LOADER):.4f}\")\n",
    "    print('Saving Model')\n",
    "    print('\\n')\n",
    "    saving_path=PATH+model_name+'_'+str(epoch)+'.pth'\n",
    "    torch.save(model.state_dict(), saving_path)\n",
    "\n",
    "    SAVE_EPOCH_LOSS.append(epoch_loss/len(TRAIN_LOADER))\n",
    "    SAVE_VAL_LOSS.append(val_loss/len(VAL_LOADER))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Save the model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T17:09:22.033483Z",
     "iopub.status.busy": "2024-11-15T17:09:22.033082Z",
     "iopub.status.idle": "2024-11-15T17:09:22.742978Z",
     "shell.execute_reply": "2024-11-15T17:09:22.741959Z",
     "shell.execute_reply.started": "2024-11-15T17:09:22.033425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name='ViT_Epoch_4.pth'\n",
    "PATH='/kaggle/working/trained_Vision_Transformer/'+model_name\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Test the Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T17:09:23.982535Z",
     "iopub.status.busy": "2024-11-15T17:09:23.982199Z",
     "iopub.status.idle": "2024-11-15T17:09:27.631224Z",
     "shell.execute_reply": "2024-11-15T17:09:27.630257Z",
     "shell.execute_reply.started": "2024-11-15T17:09:23.982505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"  # or other ViT model\n",
    "MODEL = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "num_classes = len(classes) \n",
    "\n",
    "MODEL.classifier = nn.Sequential(\n",
    "    nn.Linear(MODEL.classifier.in_features, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, num_classes),  \n",
    ")\n",
    "\n",
    "MODEL.load_state_dict(torch.load(PATH, weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T18:29:31.833216Z",
     "iopub.status.busy": "2024-11-15T18:29:31.832265Z",
     "iopub.status.idle": "2024-11-15T18:36:28.950500Z",
     "shell.execute_reply": "2024-11-15T18:36:28.949495Z",
     "shell.execute_reply.started": "2024-11-15T18:29:31.833173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50], Loss: 0.5637\n",
      "[100], Loss: 0.6513\n",
      "[150], Loss: 0.5793\n",
      "[200], Loss: 0.7420\n",
      "[250], Loss: 0.7880\n",
      "[300], Loss: 0.6474\n",
      "[350], Loss: 0.9655\n",
      "[400], Loss: 0.7393\n",
      "[450], Loss: 1.4583\n",
      "[500], Loss: 0.7041\n",
      "[550], Loss: 1.0194\n",
      "[600], Loss: 0.7351\n",
      "[650], Loss: 0.5980\n",
      "[700], Loss: 0.7031\n",
      "[750], Loss: 0.6289\n",
      "\n",
      "\n",
      "Test Loss:  0.7796184924486174\n",
      "Accuracy:  89.56\n",
      "F1 Score:  89.24\n",
      "Precision:  89.73\n",
      "Recall:  89.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "MODEL.eval()  # Set the model to evaluation mode\n",
    "\n",
    "test_loss = 0.0\n",
    "running_loss = 0.0\n",
    "\n",
    "all_preds = []  # To store all predictions\n",
    "all_labels = []  # To store all true labels\n",
    "\n",
    "for i, (frames, labels) in enumerate(TEST_LOADER):\n",
    "    frames = frames.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    batch_size, num_frames, C, H, W = frames.shape\n",
    "    frames = frames.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        outputs = model(frames)\n",
    "        logits = outputs.logits.view(batch_size, num_frames, -1)\n",
    "\n",
    "    logits = logits.squeeze(0)  # Adjust dimensions\n",
    "    labels = labels.squeeze(0)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits, labels)\n",
    "    test_loss += loss.item()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # Convert logits to predicted classes\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "    labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "    # Convert logits to predicted classes\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits, labels)\n",
    "    test_loss += loss.item()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # Store predictions and labels for metrics calculation\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"[{i+1}], Loss: {running_loss / 50:.4f}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "# Calculate average test loss\n",
    "average_test_loss = test_loss / len(TEST_LOADER)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "# Print results\n",
    "print('\\n')\n",
    "print(\"Test Loss: \",average_test_loss)\n",
    "print(\"Accuracy: \",np.round((accuracy*100),2))\n",
    "print(\"F1 Score: \",np.round((f1*100),2))\n",
    "print(\"Precision: \",np.round((precision*100),2))\n",
    "print(\"Recall: \",np.round((recall*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2749140,
     "sourceId": 4750382,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 165232,
     "modelInstanceId": 142649,
     "sourceId": 167687,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 165259,
     "modelInstanceId": 142676,
     "sourceId": 167715,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
